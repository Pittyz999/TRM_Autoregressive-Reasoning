# Tiny Recursive Model (TRM) - Autoregressive Reasoning
Adaptaci贸n del modelo Tiny Recursive Model (TRM) para razonamiento autorregresivo.

###  Tiny Recursive Model (TRM) - Autoregressive Math Reasoning
Primera versi贸n del modelo TRM adaptado con razonamiento matem谩tico autorregresivo ejecutado en un entorno de Google Colab.

###  Descripci贸n del Proyecto
Este proyecto adapta la arquitectura experimental Tiny Recursive Model (TRM) (originalmente dise帽ada para resoluci贸n de problemas de estado fijo como Sudokus y ARC-AGI) y la transforma en un Modelo de Lenguaje Autorregresivo. El objetivo es dotar al modelo de la capacidad de resolver problemas matem谩ticos en lenguaje natural (Dataset GSM8K) utilizando recursividad latente para emular "tiempo de pensamiento" (Chain of Thought interno) antes de emitir cada token.

###  Logros y Hitos T茅cnicos Alcanzados
1. Transformaci贸n Arquitect贸nica (De Matriz a Texto)
    * Atenci贸n Causal: Modificaci贸n de las capas internas del TRM para reemplazar el procesamiento bidireccional/MLP con Auto-Atenci贸n Causal (causal=True), permitiendo la generaci贸n de texto de izquierda a derecha.
    * Integraci贸n de Tokenizer: Acoplamiento exitoso del tokenizador de GPT-2 para procesar lenguaje natural en lugar de enteros fijos.
    * Memoria Din谩mica (Carry State): Redise帽o del estado latente (z_H, z_L) para que se instancie din谩micamente seg煤n la longitud de la secuencia actual, evitando problemas de dimensionalidad de tensores.

2. Correcciones de Bajo Nivel y Optimizaci贸n
    * RoPE Din谩mico (Rotary Positional Embeddings): Implementaci贸n de un parche (Monkey Patch) en la funci贸n apply_rotary_pos_emb para recortar din谩micamente las matrices de senos y cosenos, permitiendo al modelo procesar secuencias de longitud variable (desde prompts cortos de 7 tokens hasta textos de 512).
    * Sincronizaci贸n de Dispositivos: Resoluci贸n de conflictos de ejecuci贸n en PyTorch moviendo expl铆citamente los estados iniciales de memoria (creados por defecto en CPU) a la memoria CUDA.

3. Sistema de Entrenamiento Robusto
    * Smart Masking (Loss Optimization): Modificaci贸n del DataLoader para aislar el token <|endoftext|>. Se implement贸 un enmascaramiento con 铆ndice -100 para los tokens de padding subsecuentes, ense帽ando al modelo a detener la generaci贸n correctamente (evitando el colapso de "puntos infinitos").
    * Checkpointing y Resumability: Creaci贸n de un sistema de guardado continuo en Google Drive que almacena no solo los pesos del modelo, sino el estado del optimizador (AdamW), el Scheduler y la Configuraci贸n de la Arquitectura. Esto permite pausar y reanudar el entrenamiento en Colab sin p茅rdida de progreso ni explosi贸n de gradientes.
    * Human-in-the-Loop Security: Implementaci贸n de una interfaz gr谩fica (v铆a @param en Colab) con una funci贸n safety_check que compara la configuraci贸n actual con la guardada en Drive, alertando al usuario de discrepancias para evitar corromper el entrenamiento.
    * Fijaci贸n de Origen de Datos: Transici贸n al repositorio oficial "openai/gsm8k" de HuggingFace para garantizar la disponibilidad a largo plazo del dataset.

4. Interfaz de Inferencia Aislada
    * Creaci贸n de una consola de pruebas que reconstruye la arquitectura exacta del modelo bas谩ndose en el diccionario de configuraci贸n guardado en el archivo .pt, permitiendo pruebas Zero-Shot con control de temperatura (Temperature) y longitud m谩xima (Max_Tokens).

###  Estado Actual: poca 50
* P茅rdida (Loss) alcanzada: 0.1016
* Diagn贸stico actual: El modelo ha completado el ciclo de aprendizaje mec谩nico y ha demostrado que la propagaci贸n hacia atr谩s (backpropagation) a trav茅s del tiempo y los ciclos recursivos funciona perfectamente sin desbordar la VRAM de una GPU T4 (15GB).
* An谩lisis de Generalizaci贸n: Debido a la gran capacidad asignada (hidden_size=1024) frente a un dataset peque帽o (GSM8K), el modelo actual presenta un sobreajuste (Overfitting) fotogr谩fico. Ha aprendido la estructura y formato de las respuestas matem谩ticas, pero memoriz贸 los valores en lugar de generalizar la l贸gica aritm茅tica.

###  Pr贸ximos Pasos (Roadmap)
* Reducci贸n de Capacidad de Memorizaci贸n: Disminuir el hidden_size (ej. 512) para forzar al modelo a buscar patrones l贸gicos en lugar de memorizar datos.
* Aumento de Profundidad de Pensamiento: Incrementar los ciclos recursivos latentes (L_cycles=6 o 8) para mejorar la emulaci贸n del c谩lculo matem谩tico interno.
* Early Stopping: Implementar detenci贸n temprana autom谩tica para detener el entrenamiento cuando el loss alcance el "punto dulce" de generalizaci贸n (aprox. 1.2 - 1.5).

##  C贸mo usar este proyecto
1. Abre el notebook `TRM_Math_Reasoning.ipynb` en Google Colab.
2. Ejecuta la celda de configuraci贸n de carpeta y montaje de Google Drive.
3. Ejecuta las dependencias y la preparaci贸n del TRM y tokenizador.
4. Para entrenar: Ajusta los par谩metros en el formulario "Ejecutar entrenamiento con Seguridad" y dale a Play.
5. Para inferencia: Ve a la secci贸n "Consola de Pruebas", escribe tu problema en ingl茅s y ejecuta.

###  Requisitos
Este proyecto fue desarrollado en el entorno de Google Colab, en caso de querer usar el modelo en local estos son las depedencias necesarias:
* PyTorch
* Transformers (HuggingFace)
* Datasets (HuggingFace)
* einops
* tqdm
---
###  Sobre el desarrollo y uso de IA
Este proyecto fue desarrollado utilizando un enfoque de *Pair-Programming* con inteligencia artificial (Gemini). Como desarrollador principal, dirig铆 la l贸gica de adaptaci贸n, el dise帽o del flujo de trabajo y la toma de decisiones arquitect贸nicas. La IA actu贸 como mi asistente para la escritura de c贸digo estructural, el *debugging* de tensores en PyTorch y la optimizaci贸n de las limitaciones de hardware en el entorno de Google Colab.

###  Referencias y Agradecimientos
Este proyecto se apoya en el incre铆ble trabajo de la comunidad de IA. Todo el cr茅dito a los creadores originales de las siguientes herramientas y arquitecturas:

* **Arquitectura TRM:** [C贸digo base](https://github.com/SamsungSAILMontreal/TinyRecursiveModels) y conceptos del paper ["Less is More: Recursive Reasoning with Tiny Networks"](https://arxiv.org/abs/2510.04871) por Alexia Jolicoeur-Martineau.
* **Dataset GSM8K:** Conjunto de datos de problemas matem谩ticos de [OpenAI (v铆a HuggingFace)](https://huggingface.co/datasets/openai/gsm8k).
* **Tokenizador:** Tokenizador BPE pre-entrenado de [GPT-2 (OpenAI / HuggingFace)](https://huggingface.co/openai-community/gpt2).
* **Infraestructura:** Entorno de ejecuci贸n gratuito proporcionado por [Google Colab](https://colab.research.google.com/).

###  Licencia
Este proyecto est谩 bajo la Licencia MIT. Consulta el archivo `LICENSE` para m谩s detalles.
