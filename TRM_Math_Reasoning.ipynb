{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "GIUhuetbODyz",
        "hwJWuidoJ-FC",
        "lBARfU8IPJci",
        "oS1G9hs4PnTM",
        "CgOG4CK3RHWY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Modelo TRM Adaptado para Generaci√≥n de Texto y Razonamiento (GSM8K)\n",
        "\n",
        "**¬øQu√© es el TRM?** El *Tiny Recursive Model (TRM)* es una arquitectura experimental originalmente dise√±ada para resolver problemas cerrados (como Sudokus) reciclando sus propios pesos para \"pensar\" profundamente. Este *notebook* transforma esa arquitectura en un **modelo de lenguaje autorregresivo** (como GPT) capaz de leer y resolver matem√°ticas en lenguaje natural.\n",
        "\n",
        "### üõ†Ô∏è Contenido de la versi√≥n TRM_Math para Google Colab:\n",
        "\n",
        "* **Adaptaci√≥n de Arquitectura:** Integraci√≥n de Atenci√≥n Causal y memoria din√°mica para permitir la generaci√≥n de texto token a token (emulando a los modelos GPT est√°ndar).\n",
        "* **Tokenizador y RoPE Din√°mico:** Uso del tokenizador de GPT-2 y parche autom√°tico de embeddings posicionales (RoPE) para soportar *prompts* de longitud variable.\n",
        "* **Entrenamiento con GSM8K:** L√≥gica de entrenamiento optimizada con *Smart Masking* para ense√±arle al modelo cu√°ndo terminar sus respuestas matem√°ticas.\n",
        "* **Optimizado para Google Colab:** Dise√±ado para exprimir al m√°ximo la capa gratuita (**GPU T4** üí™).\n",
        "* **Checkpointing Inteligente:** Capacidad para guardar y reanudar estados de entrenamiento complejos (modelo, optimizador y configuraci√≥n) directamente en **Google Drive**. A prueba de desconexiones üòé\n",
        "* Deja tus comentarios para mejorar esta versi√≥n üòâüëç\n",
        "* **Pr√≥ximo paso:** Generalizaci√≥n del modelo (Ajuste de capacidad, recursividad y *Early Stopping*) üîç\n",
        "\n",
        "### üöÄ C√≥mo usar este proyecto\n",
        "* Abre el notebook TRM_Math_Reasoning.ipynb en Google Colab.\n",
        "* Ejecuta la celda de configuraci√≥n de carpeta y montaje de Google Drive.\n",
        "* Ejecuta las dependencias y la preparaci√≥n del TRM y tokenizador.\n",
        "* Para entrenar: Ajusta los par√°metros en el formulario \"Ejecutar entrenamiento con Seguridad\" y dale a Play.\n",
        "* Para inferencia: Ve a la secci√≥n \"Consola de Pruebas\", escribe tu problema en ingl√©s y ejecuta.\n",
        "---\n",
        "###### *Por sea caso... los emojies son m√≠os*\n",
        "### üòë"
      ],
      "metadata": {
        "id": "c66CZFgPKahv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Cargar Google Drive** üìÇ\n",
        "* Google Drive"
      ],
      "metadata": {
        "id": "GIUhuetbODyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üìÇ Montar Google Drive y Configurar Carpeta\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# @markdown Ingrese el nombre de la carpeta donde se guardar√°n los checkpoints y el modelo.\n",
        "# @markdown (Si la carpeta no existe, se crear√° autom√°ticamente).\n",
        "Project_Folder_Name = \"TRM_Math_Project\" # @param {type:\"string\"}\n",
        "\n",
        "# 1. Montar Drive (Evita pedir permisos si ya est√° montado)\n",
        "mount_path = '/content/drive'\n",
        "if not os.path.exists(mount_path):\n",
        "    print(\"üîå Conectando con Google Drive...\")\n",
        "    drive.mount(mount_path)\n",
        "else:\n",
        "    print(\"‚úÖ Google Drive ya estaba conectado.\")\n",
        "\n",
        "# 2. Configurar y Verificar Ruta\n",
        "# La ruta base en Colab siempre es /content/drive/My Drive/\n",
        "CHECKPOINT_DIR = os.path.join(mount_path, \"My Drive\", Project_Folder_Name)\n",
        "\n",
        "if os.path.exists(CHECKPOINT_DIR):\n",
        "    print(f\"üìÇ Carpeta detectada: {CHECKPOINT_DIR}\")\n",
        "    # Opcional: Listar contenido para que veas qu√© tienes\n",
        "    num_files = len(os.listdir(CHECKPOINT_DIR))\n",
        "    print(f\"   Contiene {num_files} archivos.\")\n",
        "else:\n",
        "    try:\n",
        "        os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "        print(f\"üÜï Carpeta creada exitosamente: {CHECKPOINT_DIR}\")\n",
        "    except OSError as e:\n",
        "        print(f\"‚ùå Error al crear la carpeta: {e}\")\n",
        "\n",
        "print(\"üöÄ Sistema de almacenamiento listo.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uhnF8SWnYLzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dependencias üìö**\n",
        "_Ejecutar las dependencias en orden para correr el modelo correctamente:_\n",
        "* Transformers Librerias\n",
        "* common\n",
        "* layers\n",
        "* sparse_embedding\n"
      ],
      "metadata": {
        "id": "hwJWuidoJ-FC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDDkYyLDByqf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title transformers\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title common\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def trunc_normal_init_(tensor: torch.Tensor, std: float = 1.0, lower: float = -2.0, upper: float = 2.0):\n",
        "    # NOTE: PyTorch nn.init.trunc_normal_ is not mathematically correct, the std dev is not actually the std dev of initialized tensor\n",
        "    # This function is a PyTorch version of jax truncated normal init (default init method in flax)\n",
        "    # https://github.com/jax-ml/jax/blob/main/jax/_src/random.py#L807-L848\n",
        "    # https://github.com/jax-ml/jax/blob/main/jax/_src/nn/initializers.py#L162-L199\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if std == 0:\n",
        "            tensor.zero_()\n",
        "        else:\n",
        "            sqrt2 = math.sqrt(2)\n",
        "            a = math.erf(lower / sqrt2)\n",
        "            b = math.erf(upper / sqrt2)\n",
        "            z = (b - a) / 2\n",
        "\n",
        "            c = (2 * math.pi) ** -0.5\n",
        "            pdf_u = c * math.exp(-0.5 * lower ** 2)\n",
        "            pdf_l = c * math.exp(-0.5 * upper ** 2)\n",
        "            comp_std = std / math.sqrt(1 - (upper * pdf_u - lower * pdf_l) / z - ((pdf_u - pdf_l) / z) ** 2)\n",
        "\n",
        "            tensor.uniform_(a, b)\n",
        "            tensor.erfinv_()\n",
        "            tensor.mul_(sqrt2 * comp_std)\n",
        "            tensor.clip_(lower * comp_std, upper * comp_std)\n",
        "\n",
        "    return tensor"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dLf57DI6Rk-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title layers_for_prompt\n",
        "from typing import Tuple\n",
        "import einops\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#try:\n",
        "#    from flash_attn_interface import flash_attn_func  # type: ignore[import]\n",
        "#except ImportError:\n",
        "#    # Fallback to FlashAttention 2\n",
        "#    from flash_attn import flash_attn_func  # type: ignore[import]\n",
        "from torch.nn.functional import scaled_dot_product_attention\n",
        "\n",
        "#from models.common import trunc_normal_init_\n",
        "\n",
        "\n",
        "CosSin = Tuple[torch.Tensor, torch.Tensor]\n",
        "\n",
        "\n",
        "def _find_multiple(a, b):\n",
        "    return (-(a // -b)) * b\n",
        "\n",
        "\n",
        "def rotate_half(x: torch.Tensor):\n",
        "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "\n",
        "# C√≥digo para TRM: Puzzles y Sudokus\n",
        "#def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor):\n",
        "#    # q, k: [bs, seq_len, num_heads, head_dim]\n",
        "#    # cos, sin: [seq_len, head_dim]\n",
        "#    orig_dtype = q.dtype\n",
        "#    q = q.to(cos.dtype)\n",
        "#    k = k.to(cos.dtype)\n",
        "\n",
        "#    q_embed = (q * cos.unsqueeze(-2)) + (rotate_half(q) * sin.unsqueeze(-2))\n",
        "#    k_embed = (k * cos.unsqueeze(-2)) + (rotate_half(k) * sin.unsqueeze(-2))\n",
        "\n",
        "#    return q_embed.to(orig_dtype), k_embed.to(orig_dtype)\n",
        "\n",
        "# Funci√≥n para TRM_Math: Generaci√≥n de texto y razonamiento matem√°tico b√°sico\n",
        "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor):\n",
        "    # q, k: [bs, seq_len, num_heads, head_dim]\n",
        "    # cos, sin: [MAX_seq_len, head_dim] (Aqu√≠ ven√≠a el conflicto 512 vs 7)\n",
        "\n",
        "    # 1. FIX: Detectar la longitud real de la secuencia de entrada\n",
        "    # q.shape[1] es la longitud actual (ej: 7 tokens o 512 tokens)\n",
        "    seq_len = q.shape[1]\n",
        "\n",
        "    # 2. FIX: Recortar los embeddings pre-calculados si exceden la longitud actual\n",
        "    # Esto permite que el modelo acepte prompts cortos (\"2+2\") o largos indistintamente\n",
        "    if cos.shape[0] > seq_len:\n",
        "        cos = cos[:seq_len]\n",
        "        sin = sin[:seq_len]\n",
        "\n",
        "    # 3. L√≥gica original matem√°tica (sin cambios)\n",
        "    orig_dtype = q.dtype\n",
        "    q = q.to(cos.dtype)\n",
        "    k = k.to(cos.dtype)\n",
        "\n",
        "    q_embed = (q * cos.unsqueeze(-2)) + (rotate_half(q) * sin.unsqueeze(-2))\n",
        "    k_embed = (k * cos.unsqueeze(-2)) + (rotate_half(k) * sin.unsqueeze(-2))\n",
        "\n",
        "    return q_embed.to(orig_dtype), k_embed.to(orig_dtype)\n",
        "\n",
        "\n",
        "class CastedLinear(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features: int,\n",
        "                 out_features: int,\n",
        "                 bias: bool):\n",
        "        super().__init__()\n",
        "        # Truncated LeCun normal init\n",
        "        self.weight = nn.Parameter(\n",
        "            trunc_normal_init_(torch.empty((out_features, in_features)), std=1.0 / (in_features ** 0.5))\n",
        "        )\n",
        "        self.bias = None\n",
        "        if bias:\n",
        "            # Zero init bias\n",
        "            self.bias = nn.Parameter(torch.zeros((out_features, )))\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return F.linear(input, self.weight.to(input.dtype), bias=self.bias.to(input.dtype) if self.bias is not None else None)\n",
        "\n",
        "\n",
        "class CastedEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_embeddings: int,\n",
        "                 embedding_dim: int,\n",
        "                 init_std: float,\n",
        "                 cast_to: torch.dtype):\n",
        "        super().__init__()\n",
        "        self.cast_to = cast_to\n",
        "\n",
        "        # Truncated LeCun normal init\n",
        "        self.embedding_weight = nn.Parameter(\n",
        "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std)\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return F.embedding(input, self.embedding_weight.to(self.cast_to))\n",
        "\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_position_embeddings, base, device=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # RoPE\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32, device=device) / dim))\n",
        "        t = torch.arange(max_position_embeddings, dtype=torch.float32, device=device)\n",
        "        freqs = torch.outer(t, inv_freq)\n",
        "\n",
        "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.cos_cached = nn.Buffer(emb.cos(), persistent=False)\n",
        "        self.sin_cached = nn.Buffer(emb.sin(), persistent=False)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.cos_cached, self.sin_cached\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size, head_dim, num_heads, num_key_value_heads, causal=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.head_dim = head_dim\n",
        "        self.output_size = head_dim * num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.num_key_value_heads = num_key_value_heads\n",
        "        self.causal = causal\n",
        "\n",
        "        self.qkv_proj = CastedLinear(self.hidden_size, (self.num_heads + 2 * self.num_key_value_heads) * self.head_dim, bias=False)\n",
        "        self.o_proj = CastedLinear(self.output_size, self.hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, cos_sin: CosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "\n",
        "        # hidden_states: [bs, seq_len, num_heads, head_dim]\n",
        "        qkv = self.qkv_proj(hidden_states)\n",
        "\n",
        "        # Split head\n",
        "        qkv = qkv.view(batch_size, seq_len, self.num_heads + 2 * self.num_key_value_heads, self.head_dim)\n",
        "        query = qkv[:, :, :self.num_heads]\n",
        "        key = qkv[:, :, self.num_heads: self.num_heads + self.num_key_value_heads]\n",
        "        value = qkv[:, :, self.num_heads + self.num_key_value_heads:]\n",
        "\n",
        "        # RoPE\n",
        "        if cos_sin is not None:\n",
        "            cos, sin = cos_sin\n",
        "            query, key = apply_rotary_pos_emb(query, key, cos, sin)\n",
        "\n",
        "        # flash attn\n",
        "        query, key, value = map(lambda t: einops.rearrange(t, 'B S H D -> B H S D'), (query, key, value)) # needed for scaled_dot_product_attention but not flash_attn_func\n",
        "        attn_output = scaled_dot_product_attention(query=query, key=key, value=value, is_causal=self.causal)\n",
        "        attn_output = einops.rearrange(attn_output, 'B H S D -> B S H D')\n",
        "        attn_output = attn_output.reshape(batch_size, seq_len, self.output_size)  # type: ignore\n",
        "        return self.o_proj(attn_output)\n",
        "\n",
        "class LinearSwish(nn.Module):\n",
        "    def __init__(self, hidden_size: int, reverse=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = CastedLinear(hidden_size, hidden_size, bias=False)\n",
        "        self.reverse = reverse\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.reverse:\n",
        "            return F.silu(self.linear(x))\n",
        "        else:\n",
        "            return self.linear(F.silu(x))\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def __init__(self, hidden_size: int, expansion: float):\n",
        "        super().__init__()\n",
        "        inter = _find_multiple(round(expansion * hidden_size * 2 / 3), 256)\n",
        "\n",
        "        self.gate_up_proj = CastedLinear(hidden_size, inter * 2, bias=False)\n",
        "        self.down_proj    = CastedLinear(inter, hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate, up = self.gate_up_proj(x).chunk(2, dim=-1)\n",
        "        return self.down_proj(F.silu(gate) * up)\n",
        "\n",
        "def rms_norm(hidden_states: torch.Tensor, variance_epsilon: float) -> torch.Tensor:\n",
        "    input_dtype = hidden_states.dtype\n",
        "    hidden_states = hidden_states.to(torch.float32)\n",
        "\n",
        "    variance = hidden_states.square().mean(-1, keepdim=True)\n",
        "    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n",
        "    return hidden_states.to(input_dtype)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "96vvzR8KRouy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title sparse_embedding\n",
        "from typing import Union\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.distributed as dist\n",
        "from torch.optim.optimizer import Optimizer, ParamsT\n",
        "\n",
        "#from models.common import trunc_normal_init_\n",
        "\n",
        "\n",
        "class CastedSparseEmbedding(nn.Module):\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int, batch_size: int, init_std: float, cast_to: torch.dtype):\n",
        "        super().__init__()\n",
        "        self.cast_to = cast_to\n",
        "\n",
        "        # Real Weights\n",
        "        # Truncated LeCun normal init\n",
        "        self.weights = nn.Buffer(\n",
        "            trunc_normal_init_(torch.empty((num_embeddings, embedding_dim)), std=init_std), persistent=True\n",
        "        )\n",
        "\n",
        "        # Local weights and IDs\n",
        "        # Local embeddings, with gradient, not persistent\n",
        "        self.local_weights = nn.Buffer(torch.zeros(batch_size, embedding_dim, requires_grad=True), persistent=False)\n",
        "        # Local embedding IDs, not persistent\n",
        "        self.local_ids = nn.Buffer(torch.zeros(batch_size, dtype=torch.int32), persistent=False)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.training:\n",
        "            # Test mode, no gradient\n",
        "            return self.weights[inputs].to(self.cast_to)\n",
        "\n",
        "        # Training mode, fill puzzle embedding from weights\n",
        "        with torch.no_grad():\n",
        "            self.local_weights.copy_(self.weights[inputs])\n",
        "            self.local_ids.copy_(inputs)\n",
        "\n",
        "        return self.local_weights.to(self.cast_to)\n",
        "\n",
        "\n",
        "class CastedSparseEmbeddingSignSGD_Distributed(Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: ParamsT,\n",
        "\n",
        "        world_size: int,\n",
        "        lr: Union[float, torch.Tensor] = 1e-3,\n",
        "        weight_decay: float = 1e-2,\n",
        "    ):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay,\n",
        "            world_size=world_size\n",
        "        )\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad\n",
        "    def step(self, closure=None):  # type: ignore\n",
        "        for group in self.param_groups:\n",
        "            # Find the sparse embedding weights\n",
        "            local_weights_grad = None\n",
        "            local_ids = None\n",
        "            weights = None\n",
        "\n",
        "            assert len(group[\"params\"]) == 3\n",
        "            for p in group[\"params\"]:\n",
        "                if p.requires_grad:\n",
        "                    local_weights_grad = p.grad\n",
        "                elif p.ndim == 1:\n",
        "                    local_ids = p\n",
        "                elif p.ndim == 2:\n",
        "                    weights = p\n",
        "                else:\n",
        "                    assert False\n",
        "\n",
        "            assert local_ids is not None\n",
        "            assert weights is not None\n",
        "\n",
        "            # Apply SignSGD\n",
        "            # Adam ‚âà SignSGD if gradient is very sparse\n",
        "            if local_weights_grad is not None:\n",
        "                _sparse_emb_signsgd_dist(\n",
        "                    local_weights_grad,\n",
        "                    local_ids,\n",
        "                    weights,\n",
        "\n",
        "                    lr=group[\"lr\"],\n",
        "                    weight_decay=group[\"weight_decay\"],\n",
        "                    world_size=group[\"world_size\"]\n",
        "                )\n",
        "\n",
        "\n",
        "def _sparse_emb_signsgd_dist(\n",
        "    local_weights_grad: torch.Tensor,\n",
        "    local_ids: torch.Tensor,\n",
        "    weights: torch.Tensor,\n",
        "\n",
        "    lr: float,\n",
        "    weight_decay: float,\n",
        "    world_size: int\n",
        ") -> None:\n",
        "    N, D = local_weights_grad.shape\n",
        "\n",
        "    # All-gather\n",
        "    all_weights_grad = local_weights_grad\n",
        "    all_ids = local_ids\n",
        "\n",
        "    if world_size > 1:\n",
        "        all_weights_grad = torch.empty((world_size * N, D), dtype=local_weights_grad.dtype, device=local_weights_grad.device)\n",
        "        all_ids = torch.empty(world_size * N,               dtype=local_ids.dtype,          device=local_ids.device)\n",
        "\n",
        "        dist.all_gather_into_tensor(all_weights_grad, local_weights_grad)\n",
        "        dist.all_gather_into_tensor(all_ids,          local_ids)\n",
        "\n",
        "    # Unique\n",
        "    grad_ids, inv = all_ids.unique(return_inverse=True)\n",
        "\n",
        "    grad = torch.zeros((grad_ids.shape[0], D), dtype=all_weights_grad.dtype, device=all_weights_grad.device)\n",
        "    grad.scatter_add_(0, inv.unsqueeze(-1).expand(-1, D), all_weights_grad)\n",
        "\n",
        "    # SignSGD with decoupled weight decay\n",
        "    p = weights[grad_ids]\n",
        "\n",
        "    p.mul_(1.0 - lr * weight_decay).add_(torch.sign(grad), alpha=-lr)\n",
        "\n",
        "    # Write updated slices back\n",
        "    weights[grad_ids] = p"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GfDRnGz6RvoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preparaci√≥n de TRM y Tokenizador üß†**\n",
        "* TRM Modelo Base\n",
        "* GPT-2 Tokenizer\n",
        "* Math_TRM\n",
        "* Loop de inferencia\n",
        "* Cargar tokenizador ligero\n",
        "* Prueba de verificaci√≥n"
      ],
      "metadata": {
        "id": "lBARfU8IPJci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TRM\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "import torch\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from pydantic import BaseModel\n",
        "import random\n",
        "#from models.common import trunc_normal_init_\n",
        "#from models.layers import rms_norm, LinearSwish, SwiGLU, Attention, RotaryEmbedding, CosSin, CastedEmbedding, CastedLinear\n",
        "#from models.sparse_embedding import CastedSparseEmbedding\n",
        "\n",
        "IGNORE_LABEL_ID = -100\n",
        "\n",
        "@dataclass\n",
        "class TinyRecursiveReasoningModel_ACTV1InnerCarry:\n",
        "    z_H: torch.Tensor\n",
        "    z_L: torch.Tensor\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TinyRecursiveReasoningModel_ACTV1Carry:\n",
        "    inner_carry: TinyRecursiveReasoningModel_ACTV1InnerCarry\n",
        "\n",
        "    steps: torch.Tensor\n",
        "    halted: torch.Tensor\n",
        "\n",
        "    current_data: Dict[str, torch.Tensor]\n",
        "\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1Config(BaseModel):\n",
        "    batch_size: int\n",
        "    seq_len: int\n",
        "    puzzle_emb_ndim: int = 0\n",
        "    num_puzzle_identifiers: int\n",
        "    vocab_size: int\n",
        "\n",
        "    H_cycles: int\n",
        "    L_cycles: int\n",
        "\n",
        "    H_layers: int # ignored\n",
        "    L_layers: int\n",
        "\n",
        "    # Transformer config\n",
        "    hidden_size: int\n",
        "    expansion: float\n",
        "    num_heads: int\n",
        "    pos_encodings: str\n",
        "\n",
        "    rms_norm_eps: float = 1e-5\n",
        "    rope_theta: float = 10000.0\n",
        "\n",
        "    # Halting Q-learning config\n",
        "    halt_max_steps: int\n",
        "    halt_exploration_prob: float\n",
        "\n",
        "    forward_dtype: str = \"bfloat16\"\n",
        "\n",
        "    # Alexia: added\n",
        "    mlp_t: bool = False # use mlp on L instead of transformer\n",
        "    puzzle_emb_len: int = 16 # if non-zero, its specified to this value\n",
        "    no_ACT_continue: bool =  True # No continue ACT loss, only use the sigmoid of the halt which makes much more sense\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1Block(nn.Module):\n",
        "    def __init__(self, config: TinyRecursiveReasoningModel_ACTV1Config) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "        if self.config.mlp_t:\n",
        "            self.puzzle_emb_len = -(self.config.puzzle_emb_ndim // -self.config.hidden_size) if self.config.puzzle_emb_len == 0 else self.config.puzzle_emb_len\n",
        "            self.mlp_t = SwiGLU(\n",
        "                hidden_size=self.config.seq_len + self.puzzle_emb_len, # L\n",
        "                expansion=config.expansion,\n",
        "            )\n",
        "        else:\n",
        "            self.self_attn = Attention(\n",
        "                hidden_size=config.hidden_size,\n",
        "                head_dim=config.hidden_size // config.num_heads,\n",
        "                num_heads=config.num_heads,\n",
        "                num_key_value_heads=config.num_heads,\n",
        "                causal=False\n",
        "            )\n",
        "        self.mlp = SwiGLU(\n",
        "            hidden_size=config.hidden_size,\n",
        "            expansion=config.expansion,\n",
        "        )\n",
        "        self.norm_eps = config.rms_norm_eps\n",
        "\n",
        "    def forward(self, cos_sin: CosSin, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        # B, L, D = hidden_states.shape\n",
        "        # Post Norm\n",
        "        if self.config.mlp_t:\n",
        "            hidden_states = hidden_states.transpose(1,2)\n",
        "            out = self.mlp_t(hidden_states)\n",
        "            hidden_states = rms_norm(hidden_states + out, variance_epsilon=self.norm_eps)\n",
        "            hidden_states = hidden_states.transpose(1,2)\n",
        "        else:\n",
        "            # Self Attention\n",
        "            hidden_states = rms_norm(hidden_states + self.self_attn(cos_sin=cos_sin, hidden_states=hidden_states), variance_epsilon=self.norm_eps)\n",
        "        # Fully Connected\n",
        "        out = self.mlp(hidden_states)\n",
        "        hidden_states = rms_norm(hidden_states + out, variance_epsilon=self.norm_eps)\n",
        "        return hidden_states\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1ReasoningModule(nn.Module):\n",
        "    def __init__(self, layers: List[TinyRecursiveReasoningModel_ACTV1Block]):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_injection: torch.Tensor, **kwargs) -> torch.Tensor:\n",
        "        hidden_states = hidden_states + input_injection\n",
        "        for layer in self.layers:\n",
        "            hidden_states = layer(hidden_states=hidden_states, **kwargs)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1_Inner(nn.Module):\n",
        "    def __init__(self, config: TinyRecursiveReasoningModel_ACTV1Config) -> None:\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.forward_dtype = getattr(torch, self.config.forward_dtype)\n",
        "\n",
        "        # I/O\n",
        "\n",
        "        self.embed_scale = math.sqrt(self.config.hidden_size)\n",
        "        embed_init_std = 1.0 / self.embed_scale\n",
        "\n",
        "        self.embed_tokens = CastedEmbedding(self.config.vocab_size, self.config.hidden_size, init_std=embed_init_std, cast_to=self.forward_dtype)\n",
        "        self.lm_head      = CastedLinear(self.config.hidden_size, self.config.vocab_size, bias=False)\n",
        "        self.q_head       = CastedLinear(self.config.hidden_size, 2, bias=True)\n",
        "\n",
        "        self.puzzle_emb_len = -(self.config.puzzle_emb_ndim // -self.config.hidden_size)  if self.config.puzzle_emb_len == 0 else self.config.puzzle_emb_len  # ceil div\n",
        "        if self.config.puzzle_emb_ndim > 0:\n",
        "            # Zero init puzzle embeddings\n",
        "            self.puzzle_emb = CastedSparseEmbedding(self.config.num_puzzle_identifiers, self.config.puzzle_emb_ndim,\n",
        "                                                    batch_size=self.config.batch_size, init_std=0, cast_to=self.forward_dtype)\n",
        "\n",
        "        # LM Blocks\n",
        "        if self.config.pos_encodings == \"rope\":\n",
        "            self.rotary_emb = RotaryEmbedding(dim=self.config.hidden_size // self.config.num_heads,\n",
        "                                              max_position_embeddings=self.config.seq_len + self.puzzle_emb_len,\n",
        "                                              base=self.config.rope_theta)\n",
        "        elif self.config.pos_encodings == \"learned\":\n",
        "            self.embed_pos = CastedEmbedding(self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, init_std=embed_init_std, cast_to=self.forward_dtype)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "        # Reasoning Layers\n",
        "        self.L_level = TinyRecursiveReasoningModel_ACTV1ReasoningModule(layers=[TinyRecursiveReasoningModel_ACTV1Block(self.config) for _i in range(self.config.L_layers)])\n",
        "\n",
        "        # Initial states\n",
        "        self.H_init = nn.Buffer(trunc_normal_init_(torch.empty(self.config.hidden_size, dtype=self.forward_dtype), std=1), persistent=True)\n",
        "        self.L_init = nn.Buffer(trunc_normal_init_(torch.empty(self.config.hidden_size, dtype=self.forward_dtype), std=1), persistent=True)\n",
        "\n",
        "        # Q head special init\n",
        "        # Init Q to (almost) zero for faster learning during bootstrapping\n",
        "        with torch.no_grad():\n",
        "            self.q_head.weight.zero_()\n",
        "            self.q_head.bias.fill_(-5)  # type: ignore\n",
        "\n",
        "    def _input_embeddings(self, input: torch.Tensor, puzzle_identifiers: torch.Tensor):\n",
        "        # Token embedding\n",
        "        embedding = self.embed_tokens(input.to(torch.int32))\n",
        "\n",
        "        # Puzzle embeddings\n",
        "        if self.config.puzzle_emb_ndim > 0:\n",
        "            puzzle_embedding = self.puzzle_emb(puzzle_identifiers)\n",
        "\n",
        "            pad_count = self.puzzle_emb_len * self.config.hidden_size - puzzle_embedding.shape[-1]\n",
        "            if pad_count > 0:\n",
        "                puzzle_embedding = F.pad(puzzle_embedding, (0, pad_count))\n",
        "\n",
        "            embedding = torch.cat((puzzle_embedding.view(-1, self.puzzle_emb_len, self.config.hidden_size), embedding), dim=-2)\n",
        "\n",
        "        # Position embeddings\n",
        "        if self.config.pos_encodings == \"learned\":\n",
        "            # scale by 1/sqrt(2) to maintain forward variance\n",
        "            embedding = 0.707106781 * (embedding + self.embed_pos.embedding_weight.to(self.forward_dtype))\n",
        "\n",
        "        # Scale\n",
        "        return self.embed_scale * embedding\n",
        "\n",
        "    def empty_carry(self, batch_size: int):\n",
        "        return TinyRecursiveReasoningModel_ACTV1InnerCarry(\n",
        "            z_H=torch.empty(batch_size, self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, dtype=self.forward_dtype),\n",
        "            z_L=torch.empty(batch_size, self.config.seq_len + self.puzzle_emb_len, self.config.hidden_size, dtype=self.forward_dtype),\n",
        "        )\n",
        "\n",
        "    def reset_carry(self, reset_flag: torch.Tensor, carry: TinyRecursiveReasoningModel_ACTV1InnerCarry):\n",
        "        return TinyRecursiveReasoningModel_ACTV1InnerCarry(\n",
        "            z_H=torch.where(reset_flag.view(-1, 1, 1), self.H_init, carry.z_H),\n",
        "            z_L=torch.where(reset_flag.view(-1, 1, 1), self.L_init, carry.z_L),\n",
        "        )\n",
        "\n",
        "    def forward(self, carry: TinyRecursiveReasoningModel_ACTV1InnerCarry, batch: Dict[str, torch.Tensor]) -> Tuple[TinyRecursiveReasoningModel_ACTV1InnerCarry, torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        seq_info = dict(\n",
        "            cos_sin=self.rotary_emb() if hasattr(self, \"rotary_emb\") else None,\n",
        "        )\n",
        "\n",
        "        # Input encoding\n",
        "        input_embeddings = self._input_embeddings(batch[\"inputs\"], batch[\"puzzle_identifiers\"])\n",
        "\n",
        "        # Forward iterations\n",
        "        it = 0\n",
        "        z_H, z_L = carry.z_H, carry.z_L\n",
        "        # H_cycles-1 without grad\n",
        "        with torch.no_grad():\n",
        "            for _H_step in range(self.config.H_cycles-1):\n",
        "                for _L_step in range(self.config.L_cycles):\n",
        "                    z_L = self.L_level(z_L, z_H + input_embeddings, **seq_info)\n",
        "                z_H = self.L_level(z_H, z_L, **seq_info)\n",
        "        # 1 with grad\n",
        "        for _L_step in range(self.config.L_cycles):\n",
        "            z_L = self.L_level(z_L, z_H + input_embeddings, **seq_info)\n",
        "        z_H = self.L_level(z_H, z_L, **seq_info)\n",
        "\n",
        "        # LM Outputs\n",
        "        new_carry = TinyRecursiveReasoningModel_ACTV1InnerCarry(z_H=z_H.detach(), z_L=z_L.detach())  # New carry no grad\n",
        "        output = self.lm_head(z_H)[:, self.puzzle_emb_len:]\n",
        "        q_logits = self.q_head(z_H[:, 0]).to(torch.float32) # Q-head; uses the first puzzle_emb position\n",
        "        return new_carry, output, (q_logits[..., 0], q_logits[..., 1])\n",
        "\n",
        "\n",
        "class TinyRecursiveReasoningModel_ACTV1(nn.Module):\n",
        "    \"\"\"ACT wrapper.\"\"\"\n",
        "\n",
        "    def __init__(self, config_dict: dict):\n",
        "        super().__init__()\n",
        "        self.config = TinyRecursiveReasoningModel_ACTV1Config(**config_dict)\n",
        "        self.inner = TinyRecursiveReasoningModel_ACTV1_Inner(self.config)\n",
        "\n",
        "    @property\n",
        "    def puzzle_emb(self):\n",
        "        return self.inner.puzzle_emb\n",
        "\n",
        "    def initial_carry(self, batch: Dict[str, torch.Tensor]):\n",
        "        batch_size = batch[\"inputs\"].shape[0]\n",
        "\n",
        "        return TinyRecursiveReasoningModel_ACTV1Carry(\n",
        "            inner_carry=self.inner.empty_carry(batch_size),  # Empty is expected, it will be reseted in first pass as all sequences are halted.\n",
        "\n",
        "            steps=torch.zeros((batch_size, ), dtype=torch.int32),\n",
        "            halted=torch.ones((batch_size, ), dtype=torch.bool),  # Default to halted\n",
        "\n",
        "            current_data={k: torch.empty_like(v) for k, v in batch.items()}\n",
        "        )\n",
        "\n",
        "    def forward(self, carry: TinyRecursiveReasoningModel_ACTV1Carry, batch: Dict[str, torch.Tensor]) -> Tuple[TinyRecursiveReasoningModel_ACTV1Carry, Dict[str, torch.Tensor]]:\n",
        "\n",
        "        # Update data, carry (removing halted sequences)\n",
        "        new_inner_carry = self.inner.reset_carry(carry.halted, carry.inner_carry)\n",
        "\n",
        "        new_steps = torch.where(carry.halted, 0, carry.steps)\n",
        "\n",
        "        new_current_data = {k: torch.where(carry.halted.view((-1, ) + (1, ) * (batch[k].ndim - 1)), batch[k], v) for k, v in carry.current_data.items()}\n",
        "\n",
        "        # Forward inner model\n",
        "        new_inner_carry, logits, (q_halt_logits, q_continue_logits) = self.inner(new_inner_carry, new_current_data)\n",
        "\n",
        "        outputs = {\n",
        "            \"logits\": logits,\n",
        "            \"q_halt_logits\": q_halt_logits,\n",
        "            \"q_continue_logits\": q_continue_logits\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Step\n",
        "            new_steps = new_steps + 1\n",
        "            is_last_step = new_steps >= self.config.halt_max_steps\n",
        "\n",
        "            halted = is_last_step\n",
        "\n",
        "            # if training, and ACT is enabled\n",
        "            if self.training and (self.config.halt_max_steps > 1):\n",
        "\n",
        "                # Halt signal\n",
        "                # NOTE: During evaluation, always use max steps, this is to guarantee the same halting steps inside a batch for batching purposes\n",
        "\n",
        "                if self.config.no_ACT_continue:\n",
        "                    halted = halted | (q_halt_logits > 0)\n",
        "                else:\n",
        "                    halted = halted | (q_halt_logits > q_continue_logits)\n",
        "\n",
        "                # Exploration\n",
        "                min_halt_steps = (torch.rand_like(q_halt_logits) < self.config.halt_exploration_prob) * torch.randint_like(new_steps, low=2, high=self.config.halt_max_steps + 1)\n",
        "                halted = halted & (new_steps >= min_halt_steps)\n",
        "\n",
        "                if not self.config.no_ACT_continue:\n",
        "                    # Compute target Q\n",
        "                    # NOTE: No replay buffer and target networks for computing target Q-value.\n",
        "                    # As batch_size is large, there're many parallel envs.\n",
        "                    # Similar concept as PQN https://arxiv.org/abs/2407.04811\n",
        "                    _, _, (next_q_halt_logits, next_q_continue_logits), _, _ = self.inner(new_inner_carry, new_current_data)\n",
        "                    outputs[\"target_q_continue\"] = torch.sigmoid(torch.where(is_last_step, next_q_halt_logits, torch.maximum(next_q_halt_logits, next_q_continue_logits)))\n",
        "\n",
        "        return TinyRecursiveReasoningModel_ACTV1Carry(new_inner_carry, new_steps, halted, new_current_data), outputs"
      ],
      "metadata": {
        "id": "t-rmi-VcDchL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# @title GPT-2 Tokenizer\n",
        "# @markdown ##### Modifique los par√°metros con cuidado.\n",
        "# @markdown Riesgo de sobrecargar la VRAM y no poder ejecutarse el modelo o el entrenamiento.\n",
        "\n",
        "#m_batch_size = 1 # @param {type:\"number\"}\n",
        "#m_seq_len = 512 # @param {type:\"number\"}\n",
        "#m_num_puzzle_identifiers = 0 # @param {type:\"number\"}\n",
        "#m_vocab_size = \"tokenizer.vocab_size\" # @param {type:\"string\"}\n",
        "#m_H_cycles = 2 # @param {type:\"number\"}\n",
        "#m_L_cycles = 6 # @param {type:\"number\"}\n",
        "#m_H_layers = 0 # @param {type:\"number\"}\n",
        "#m_L_layers = 2 # @param {type:\"number\"}\n",
        "#m_hidden_size = 1024 # @param {type:\"number\"}\n",
        "#m_expansion = 4.0 # @param\n",
        "#m_num_heads = 16 # @param {type:\"number\"}\n",
        "#m_pos_encodings = \"rope\" # @param {typer:\"string\"}\n",
        "#m_halt_max_steps = 16 # @param {type:\"number\"}\n",
        "#m_halt_exploration_prob = 0.0 # @param {type:\"number\"}\n",
        "#m_forward_dtype = \"\\\"float32\\\"\" # @param {type:\"string\"}\n",
        "\n",
        "class MathTRMConfig(TinyRecursiveReasoningModel_ACTV1Config):\n",
        "    def __init__(self, vocab_size, **kwargs):\n",
        "        # Configuraciones forzadas para funcionamiento tipo GPT\n",
        "        kwargs[\"vocab_size\"] = vocab_size\n",
        "        kwargs[\"puzzle_emb_ndim\"] = 0   # Desactivamos embeddings de puzzle\n",
        "        kwargs[\"puzzle_emb_len\"] = 0\n",
        "        kwargs[\"mlp_t\"] = False         # IMPORTANTE: False para usar Attention class\n",
        "        kwargs[\"pos_encodings\"] = \"rope\"\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "def get_math_config(tokenizer):\n",
        "    return {\n",
        "        \"batch_size\": 1,      # No tocar\n",
        "        \"seq_len\": 512,       # Contexto suficiente para preguntas matem√°ticas\n",
        "        \"num_puzzle_identifiers\": 0,\n",
        "        \"vocab_size\": tokenizer.vocab_size,\n",
        "        \"H_cycles\": 2,        # Ciclos de refinamiento de respuesta (T)\n",
        "        \"L_cycles\": 6,        # Ciclos de razonamiento latente (n)\n",
        "        \"H_layers\": 0,        # No usado\n",
        "        \"L_layers\": 2,        # Mantenemos la red \"Tiny\"\n",
        "        \"hidden_size\": 1024,  # Dimensi√≥n similar a GPT-2 Small\n",
        "        \"expansion\": 4.0,\n",
        "        \"num_heads\": 16,\n",
        "        \"pos_encodings\": \"rope\",\n",
        "        \"halt_max_steps\": 16, # M√°ximo pasos de \"pensamiento\" ACT\n",
        "        \"halt_exploration_prob\": 0.0,\n",
        "        \"forward_dtype\": \"float32\"\n",
        "    }"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RF13CxzX3qAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Math_TRM\n",
        "class MathTRM(nn.Module):\n",
        "    def __init__(self, tokenizer):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        config_dict = get_math_config(tokenizer)\n",
        "\n",
        "        # Aseguramos que el puzzle_len sea 0 en la configuraci√≥n interna\n",
        "        config_dict['puzzle_emb_len'] = 0\n",
        "\n",
        "        self.model = TinyRecursiveReasoningModel_ACTV1(config_dict)\n",
        "\n",
        "        # --- PARCHE DE CAUSALIDAD (Necesario para texto) ---\n",
        "        for layer in self.model.inner.L_level.layers:\n",
        "            if hasattr(layer, 'self_attn'):\n",
        "                layer.self_attn.causal = True\n",
        "\n",
        "    def forward(self, input_ids, carry=None):\n",
        "        device = input_ids.device\n",
        "        # Capturamos el tama√±o REAL de la entrada actual (ej: 7 tokens)\n",
        "        batch_size, current_seq_len = input_ids.shape\n",
        "\n",
        "        batch = {\n",
        "            \"inputs\": input_ids,\n",
        "            \"puzzle_identifiers\": torch.zeros((batch_size, 1), dtype=torch.long).to(device)\n",
        "        }\n",
        "\n",
        "        if carry is None:\n",
        "            # --- CORRECCI√ìN CR√çTICA ---\n",
        "            # En lugar de usar initial_carry() (que crea un tensor de 512+16),\n",
        "            # creamos manualmente una memoria del tama√±o EXACTO de la entrada (ej: 7).\n",
        "\n",
        "            dtype = self.model.inner.forward_dtype # Generalmente float32 o bfloat16\n",
        "            hidden_size = self.model.config.hidden_size\n",
        "\n",
        "            # 1. Crear tensores de memoria vac√≠os con tama√±o [Batch, Current_Len, Hidden]\n",
        "            inner_carry = TinyRecursiveReasoningModel_ACTV1InnerCarry(\n",
        "                z_H=torch.zeros(batch_size, current_seq_len, hidden_size, device=device, dtype=dtype),\n",
        "                z_L=torch.zeros(batch_size, current_seq_len, hidden_size, device=device, dtype=dtype),\n",
        "            )\n",
        "\n",
        "            # 2. Empaquetar en el objeto de transporte\n",
        "            carry = TinyRecursiveReasoningModel_ACTV1Carry(\n",
        "                inner_carry=inner_carry,\n",
        "                steps=torch.zeros((batch_size,), dtype=torch.int32, device=device),\n",
        "                halted=torch.ones((batch_size,), dtype=torch.bool, device=device), # True para que reset_carry inicie los pesos\n",
        "                current_data={k: v.to(device) for k, v in batch.items()}\n",
        "            )\n",
        "\n",
        "        # Pase recursivo\n",
        "        new_carry, outputs = self.model(carry, batch)\n",
        "\n",
        "        return outputs[\"logits\"], new_carry"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WGXpW2SY_-H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loop de Inferencia\n",
        "def solve_math_problem(prompt, model, max_new_tokens=100):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    tokenizer = model.tokenizer\n",
        "\n",
        "    # Tokenizar entrada\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "    print(f\"Pregunta: {prompt}\\nRespuesta TRM: \", end=\"\", flush=True)\n",
        "\n",
        "    generated_ids = input_ids\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # 1. Forward Pass\n",
        "            # Enviamos toda la secuencia actual. El modelo \"pensar√°\" recursivamente\n",
        "            # sobre ella (L_cycles * H_cycles) antes de darnos logits.\n",
        "            logits, _ = model(generated_ids, carry=None)\n",
        "\n",
        "            # 2. Predecir siguiente token (usamos el √∫ltimo de la secuencia)\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
        "\n",
        "            # 3. Decodificar y mostrar\n",
        "            word = tokenizer.decode(next_token[0])\n",
        "            print(word, end=\"\", flush=True)\n",
        "\n",
        "            # 4. Actualizar secuencia\n",
        "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
        "\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rNd-KPptALYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cargar tokenizador ligero\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Iniciar modelo\n",
        "model = MathTRM(tokenizer).to(\"cuda\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dUQZoEuhAj9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Prueba de verificaci√≥n\n",
        "pregunta = \"Cuanto es 2 + 2?\"\n",
        "respuesta = solve_math_problem(pregunta, model)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hIOa07vnRXD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Etapa de entrenamiento ‚òùü§ì**\n",
        "* üìö Cargar y Procesar Dataset (GSM8K)\n",
        "* Funci√≥n de entrenamiento y verificaci√≥n de seguridad\n",
        "* Ejecutar entrenamiento con Seguridad üõ°Ô∏è\n"
      ],
      "metadata": {
        "id": "oS1G9hs4PnTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üìö Cargar y Procesar Dataset (GSM8K)\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "# 1. Definici√≥n de la Clase\n",
        "class GSM8KDataset(Dataset):\n",
        "    def __init__(self, tokenizer, split=\"train\", max_length=512):\n",
        "        print(f\"üì• Descargando/Cargando dataset GSM8K ({split})...\")\n",
        "\n",
        "        # --- FIX: Usamos el ID oficial 'openai/gsm8k' ---\n",
        "        try:\n",
        "            self.dataset = load_dataset(\"openai/gsm8k\", \"main\", split=split)\n",
        "        except Exception as e:\n",
        "            # Fallback por si acaso, aunque el anterior es el oficial\n",
        "            print(f\"‚ö†Ô∏è Error cargando openai/gsm8k: {e}. Intentando ruta alternativa...\")\n",
        "            self.dataset = load_dataset(\"gsm8k\", \"main\", split=split)\n",
        "\n",
        "        print(f\"‚úÖ ¬°Dataset listo! {len(self.dataset)} ejemplos procesados.\")\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        # Formato de Chat: Pregunta -> Respuesta\n",
        "        text = f\"Pregunta: {item['question']}\\nRespuesta: {item['answer']}<|endoftext|>\"\n",
        "\n",
        "        encodings = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encodings[\"input_ids\"].squeeze(0)\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # M√ÅSCARA INTELIGENTE (Smart Masking)\n",
        "        # Ignoramos el padding para que el modelo no aprenda a generar espacios vac√≠os\n",
        "        eos_mask = (input_ids == self.tokenizer.eos_token_id)\n",
        "        if eos_mask.any():\n",
        "            # Encontrar el primer EOS (donde acaba la frase real)\n",
        "            first_eos_idx = torch.where(eos_mask)[0][0]\n",
        "\n",
        "            # Si hay espacio despu√©s del EOS, lo marcamos con -100 (ignorar)\n",
        "            if first_eos_idx + 1 < len(labels):\n",
        "                labels[first_eos_idx + 1:] = -100\n",
        "\n",
        "        return input_ids, labels\n",
        "\n",
        "# 2. Instanciaci√≥n (Solo preparamos los datos)\n",
        "if 'tokenizer' not in globals():\n",
        "    from transformers import AutoTokenizer\n",
        "    # Usamos GPT-2 tokenizer por ser ligero y eficaz para matem√°ticas\n",
        "    print(\"‚öôÔ∏è Cargando Tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Instanciamos el objeto Dataset\n",
        "train_dataset = GSM8KDataset(tokenizer, split=\"train\")\n",
        "\n",
        "# Verificaci√≥n de integridad\n",
        "if len(train_dataset) == 0:\n",
        "    raise ValueError(\"üî¥ ERROR CR√çTICO: El dataset parece vac√≠o.\")\n",
        "else:\n",
        "    print(f\"‚ú® Todo listo. El dataset 'train_dataset' est√° disponible para el entrenamiento.\")\n",
        "    # NOTA: No creamos el DataLoader aqu√≠.\n",
        "    # El 'batch_size' se definir√° din√°micamente en el formulario de entrenamiento."
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ug9Kic2kFOF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import time\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import tqdm\n",
        "\n",
        "# @title Funci√≥n de entrenamiento y verificaci√≥n de seguridad\n",
        "# --- MODIFICACI√ìN 1: Guardamos tambi√©n la configuraci√≥n ---\n",
        "def save_checkpoint(path, model, optimizer, scheduler, epoch, loss, config):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': loss,\n",
        "        'config': config # <--- Guardamos los par√°metros usados\n",
        "    }, path)\n",
        "    print(f\"   üíæ Checkpoint guardado.\")\n",
        "\n",
        "# --- MODIFICACI√ìN 2: Nueva funci√≥n de verificaci√≥n ---\n",
        "def safety_check(saved_config, current_config):\n",
        "    \"\"\"Compara configuraciones y pide confirmaci√≥n al usuario si hay cambios.\"\"\"\n",
        "    if saved_config is None:\n",
        "        print(\"‚ö†Ô∏è Checkpoint antiguo sin datos de configuraci√≥n. Se usar√°n los nuevos par√°metros.\")\n",
        "        return current_config, True # Continuar\n",
        "\n",
        "    diffs = []\n",
        "    # Comparamos claves relevantes\n",
        "    keys_to_check = ['lr', 'batch_size', 'accumulation_steps', 'total_epochs']\n",
        "\n",
        "    for key in keys_to_check:\n",
        "        old_val = saved_config.get(key)\n",
        "        new_val = current_config.get(key)\n",
        "        # Usamos str() para evitar problemas de redondeo con floats al comparar\n",
        "        if str(old_val) != str(new_val):\n",
        "            diffs.append((key, old_val, new_val))\n",
        "\n",
        "    if not diffs:\n",
        "        print(\"‚úÖ Verificaci√≥n de seguridad: Par√°metros id√©nticos. Continuando...\")\n",
        "        return current_config, True\n",
        "\n",
        "    # Si hay diferencias, activamos la ALERTA\n",
        "    print(\"\\n\" + \"!\"*60)\n",
        "    print(\"üõë ALERTA DE SEGURIDAD: Los par√°metros han cambiado\")\n",
        "    print(\"!\"*60)\n",
        "    print(f\"{'PAR√ÅMETRO':<20} | {'GUARDADO':<15} | {'NUEVO (@param)':<15}\")\n",
        "    print(\"-\" * 56)\n",
        "    for key, old, new in diffs:\n",
        "        print(f\"{key:<20} | {str(old):<15} | {str(new):<15}\")\n",
        "    print(\"-\" * 56)\n",
        "\n",
        "    print(\"\\nOpciones:\")\n",
        "    print(\"  [Y]  Usar los NUEVOS par√°metros (Sobreescribe la configuraci√≥n anterior)\")\n",
        "    print(\"  [N]  Usar los par√°metros GUARDADOS (Ignora lo que pusiste en @param)\")\n",
        "    print(\"  [S]  STOP / Abortar (Para corregir manualmente)\")\n",
        "\n",
        "    while True:\n",
        "        choice = input(\"\\n¬øDesea continuar con los NUEVOS par√°metros? (Y/N/S): \").strip().upper()\n",
        "        if choice == 'Y':\n",
        "            print(\"üëâ Has elegido: NUEVOS par√°metros.\")\n",
        "            return current_config, True\n",
        "        elif choice == 'N':\n",
        "            print(\"üëâ Has elegido: RESTAURAR par√°metros guardados.\")\n",
        "            return saved_config, True\n",
        "        elif choice == 'S' or choice == 'STOP':\n",
        "            print(\"üõë Ejecuci√≥n abortada por el usuario.\")\n",
        "            return None, False\n",
        "        else:\n",
        "            print(\"Opci√≥n no v√°lida. Escribe Y, N o S.\")\n",
        "\n",
        "def train_resumable_safe(model, train_dataset, current_config):\n",
        "    # Desempaquetar config actual\n",
        "    total_epochs = current_config['total_epochs']\n",
        "    batch_size = current_config['batch_size']\n",
        "    accumulation_steps = current_config['accumulation_steps']\n",
        "    lr = current_config['lr']\n",
        "\n",
        "    # Rutas\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, \"trm_gsm8k_latest.pt\")\n",
        "    best_model_path = os.path.join(CHECKPOINT_DIR, \"trm_gsm8k_best.pt\")\n",
        "\n",
        "    # Loader inicial (se actualizar√° si cambiamos batch_size en el check)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.1)\n",
        "\n",
        "    # Scheduler placeholder (se recalcular√°)\n",
        "    total_steps = total_epochs * len(train_loader)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "    loss_fct = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    start_epoch = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # --- L√ìGICA DE CARGA Y SEGURIDAD ---\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"üîÑ Checkpoint detectado. Analizando...\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "        saved_config = checkpoint.get('config', None)\n",
        "\n",
        "        # === EJECUCI√ìN DEL CHECK ===\n",
        "        final_config, should_continue = safety_check(saved_config, current_config)\n",
        "\n",
        "        if not should_continue:\n",
        "            return None # Abortar\n",
        "\n",
        "        # Actualizar variables locales con la decisi√≥n tomada\n",
        "        batch_size = final_config['batch_size']\n",
        "        accumulation_steps = final_config['accumulation_steps']\n",
        "        lr = final_config['lr']\n",
        "        total_epochs = final_config['total_epochs']\n",
        "\n",
        "        # Re-crear loader y scheduler si cambiaron los par√°metros\n",
        "        if final_config != current_config or saved_config is not None:\n",
        "             train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "             total_steps = total_epochs * len(train_loader)\n",
        "             # Importante: Reinstanciar scheduler con nuevos steps totales\n",
        "             scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)\n",
        "\n",
        "        # Cargar estados\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        # Solo cargamos scheduler si no cambiamos dr√°sticamente la duraci√≥n\n",
        "        # Si cambiamos epochs, es mejor dejar el scheduler nuevo\n",
        "        if saved_config and saved_config['total_epochs'] == total_epochs:\n",
        "             scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        best_loss = checkpoint.get('loss', float('inf'))\n",
        "\n",
        "        # FORZAR LR (Si elegimos nuevos par√°metros o viejos, aseguramos que el optimizador obedezca)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        print(f\"‚úÖ Reanudando en √âpoca {start_epoch} | Batch {batch_size} | LR {lr}\")\n",
        "    else:\n",
        "        print(\"üÜï Iniciando entrenamiento nuevo (Sin checkpoint previo).\")\n",
        "\n",
        "    if start_epoch >= total_epochs:\n",
        "        print(\"üéâ ¬°Entrenamiento completado! Aumenta 'Epochs' si deseas continuar.\")\n",
        "        return model\n",
        "\n",
        "    # --- BUCLE DE ENTRENAMIENTO ---\n",
        "    model.train()\n",
        "    print(f\"üöÄ Corriendo... (Guardando configuraci√≥n en cada paso)\")\n",
        "\n",
        "    # Configuraci√≥n final a guardar\n",
        "    active_config = {\n",
        "        'total_epochs': total_epochs,\n",
        "        'batch_size': batch_size,\n",
        "        'accumulation_steps': accumulation_steps,\n",
        "        'lr': lr\n",
        "    }\n",
        "\n",
        "    for epoch in range(start_epoch, total_epochs):\n",
        "        loop = tqdm.tqdm(train_loader, desc=f\"Epoca {epoch+1}/{total_epochs}\")\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_idx, (input_ids, labels) in enumerate(loop):\n",
        "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "\n",
        "            logits, _ = model(input_ids, carry=None)\n",
        "\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "            loss = loss / accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            current_loss = loss.item() * accumulation_steps\n",
        "            epoch_loss += current_loss\n",
        "            loop.set_postfix(loss=f\"{current_loss:.4f}\")\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"   üìâ Fin Epoca {epoch+1} - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Pasamos active_config al guardar\n",
        "        save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, avg_loss, active_config)\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"   üèÜ R√©cord guardado.\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YKiKxQCPF14w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ejecutar entrenamiento con Seguridad üõ°Ô∏è\n",
        "# @markdown ## **Par√°metros de control**\n",
        "# @markdown * Ingrese los par√°metros para empezar entrenamiento en etapas.\n",
        "# @markdown * Debido a las limitaciones de uso del entorno de ejecuci√≥n y disponibilidad de GPUs, el entrenamiento se guarda cada etapa con duraci√≥n promedio de 35-45 minutos en GPU T4 con la configuraci√≥n actual del modelo y par√°metros usados en el entrenamiento.\n",
        "# @markdown * Puede volver a continuar el entrenamiento con los mismos par√°metros para terminar todas las \"Epocas\" de entrenamiento.\n",
        "# @markdown * Pruebe sus propios par√°metros de modelo y entrenamiento para calcular el tiempo de cada √©poca y calcular cuanto aproximadamente durar√° todo el entrenamiento. BAJO TU PROPIO RIESGO el aumentar demasiado los par√°metros de modelo o entrenamiento.\n",
        "\n",
        "# Variables\n",
        "Epochs = 50 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "Batch_Size = 8 # @param {type:\"slider\", min:1, max:32, step:1}\n",
        "Accumulation_Steps = 4 # @param {type:\"slider\", min:1, max:16, step:1}\n",
        "Learning_Rate = 1e-4 # @param {type:\"number\"}\n",
        "\n",
        "# Empaquetamos la configuraci√≥n actual del usuario\n",
        "current_user_config = {\n",
        "    'total_epochs': Epochs,\n",
        "    'batch_size': Batch_Size,\n",
        "    'accumulation_steps': Accumulation_Steps,\n",
        "    'lr': Learning_Rate\n",
        "}\n",
        "\n",
        "# Ejecutamos con la nueva funci√≥n segura\n",
        "# (Nota: Aseg√∫rate de haber ejecutado la celda anterior con la nueva funci√≥n train_resumable_safe)\n",
        "if 'model' in globals():\n",
        "    model = train_resumable_safe(model, train_dataset, current_user_config)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Primero debes instanciar el 'model' y cargar el 'train_dataset'.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Rt7cYiBMn2K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prueba de fuego ü§î**\n",
        "Ejecuta el modelo con un prompt de matem√°ticas en lenguaje natural, en ingl√©s."
      ],
      "metadata": {
        "id": "CgOG4CK3RHWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Funciones de carga, inferencia y seguridad üõ°Ô∏è\n",
        "import torch\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# --- FUNCIONES AUXILIARES ---\n",
        "\n",
        "def load_trained_model(device, use_latest=False):\n",
        "    \"\"\"\n",
        "    Carga el modelo.\n",
        "    use_latest=True  -> Carga el √∫ltimo checkpoint (ideal para ver progreso).\n",
        "    use_latest=False -> Carga el mejor modelo validado (ideal para demos).\n",
        "    \"\"\"\n",
        "    best_path = os.path.join(CHECKPOINT_DIR, \"trm_gsm8k_best.pt\")\n",
        "    latest_path = os.path.join(CHECKPOINT_DIR, \"trm_gsm8k_latest.pt\")\n",
        "\n",
        "    # L√≥gica de selecci√≥n\n",
        "    if use_latest:\n",
        "        path_to_load = latest_path if os.path.exists(latest_path) else best_path\n",
        "        print(\"Build: Cargando versi√≥n M√ÅS RECIENTE (Latest)...\")\n",
        "    else:\n",
        "        path_to_load = best_path if os.path.exists(best_path) else latest_path\n",
        "        print(\"Build: Cargando versi√≥n MEJOR EVALUADA (Best)...\")\n",
        "\n",
        "    if not os.path.exists(path_to_load):\n",
        "        raise FileNotFoundError(f\"‚ùå No se encontr√≥ modelo en: {path_to_load}\")\n",
        "\n",
        "    checkpoint = torch.load(path_to_load, map_location=device)\n",
        "\n",
        "    # 1. Recuperar Configuraci√≥n Guardada\n",
        "    saved_config = checkpoint.get('config', None)\n",
        "\n",
        "    # --- FIX: Inyecci√≥n de Configuraci√≥n ---\n",
        "    # Para que MathTRM use la config guardada (ej. hidden_size=1024) y no la default.\n",
        "    # Guardamos la funci√≥n original para restaurarla despu√©s.\n",
        "    original_get_config = globals().get('get_math_config')\n",
        "\n",
        "    if saved_config:\n",
        "        print(f\"‚öôÔ∏è Configuraci√≥n recuperada: H={saved_config.get('hidden_size')} | L_cycles={saved_config.get('L_cycles')}\")\n",
        "\n",
        "        # Creamos una funci√≥n temporal que devuelve TU configuraci√≥n guardada\n",
        "        def patched_get_config(tokenizer):\n",
        "            base_config = original_get_config(tokenizer)\n",
        "            base_config.update(saved_config) # Sobreescribimos con lo guardado\n",
        "            return base_config\n",
        "\n",
        "        # Reemplazamos globalmente\n",
        "        globals()['get_math_config'] = patched_get_config\n",
        "\n",
        "    try:\n",
        "        # Instanciamos el modelo (usar√° la config parcheada)\n",
        "        model = MathTRM(tokenizer)\n",
        "    finally:\n",
        "        # Restauramos la funci√≥n original pase lo que pase\n",
        "        if saved_config and original_get_config:\n",
        "            globals()['get_math_config'] = original_get_config\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # 2. Cargar Pesos\n",
        "    try:\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    except RuntimeError as e:\n",
        "        print(f\"‚ö†Ô∏è Error cargando pesos: {e}\")\n",
        "        print(\"Posible causa: Cambiaste la arquitectura (capas, tama√±os) y no coincide con el checkpoint.\")\n",
        "        raise e\n",
        "\n",
        "    model.eval()\n",
        "    return model, checkpoint['epoch']\n",
        "\n",
        "def generate_answer(model, prompt, max_tokens=100, temperature=0.7):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    formatted_prompt = f\"Pregunta: {prompt}\\nRespuesta:\"\n",
        "\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    generated_ids = input_ids\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_tokens):\n",
        "            logits, _ = model(generated_ids, carry=None)\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "\n",
        "            if temperature > 0:\n",
        "                probs = torch.softmax(next_token_logits / temperature, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
        "\n",
        "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    full_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return full_text.split(\"Respuesta:\")[-1].strip()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UPO9MaTeKNrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üß™ Consola de Pruebas (Inferencia Mejorada) - Prompt de control\n",
        "# --- INTERFAZ ---\n",
        "# @markdown ### Par√°metros de Prueba\n",
        "Prompt = \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\" # @param {type:\"string\"}\n",
        "Max_Tokens = 100 # @param {type:\"slider\", min:10, max:512, step:10}\n",
        "Temperature = 0.1 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "Version_Modelo = \"Latest (Progreso actual)\" # @param [\"Best (Mejor hist√≥rico)\", \"Latest (Progreso actual)\"]\n",
        "\n",
        "# L√≥gica de carga\n",
        "use_latest = (Version_Modelo == \"Latest (Progreso actual)\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if 'tokenizer' not in globals():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Cargar modelo (siempre recargamos para asegurar que sea la versi√≥n elegida)\n",
        "# Nota: Si est√°s entrenando en la misma sesi√≥n, esto pausar√° brevemente el uso de GPU.\n",
        "try:\n",
        "    model_infer, epoch_loaded = load_trained_model(device, use_latest=use_latest)\n",
        "    print(f\"‚úÖ Modelo cargado (√âpoca {epoch_loaded})\")\n",
        "\n",
        "    print(f\"\\nüß† Generando respuesta...\\n{'-'*30}\")\n",
        "    res = generate_answer(model_infer, Prompt, Max_Tokens, Temperature)\n",
        "    print(f\"üìù Pregunta: {Prompt}\")\n",
        "    print(f\"üí° Respuesta:\\n{res}\")\n",
        "    print(f\"{'-'*30}\")\n",
        "\n",
        "    # Limpieza para liberar VRAM si es necesario volver a entrenar\n",
        "    del model_infer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FR0huhk8NIfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title üß™ Consola de Pruebas\n",
        "# --- INTERFAZ ---\n",
        "# @markdown ### Par√°metros de Prueba\n",
        "Prompt = \"A farmer has 10 cows and 5 horses. If 2 cows escape, how many horses are left?\" # @param {type:\"string\"}\n",
        "Max_Tokens = 100 # @param {type:\"slider\", min:10, max:512, step:10}\n",
        "Temperature = 0.1 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "Version_Modelo = \"Latest (Progreso actual)\" # @param [\"Best (Mejor hist√≥rico)\", \"Latest (Progreso actual)\"]\n",
        "\n",
        "# L√≥gica de carga\n",
        "use_latest = (Version_Modelo == \"Latest (Progreso actual)\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if 'tokenizer' not in globals():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Cargar modelo (siempre recargamos para asegurar que sea la versi√≥n elegida)\n",
        "# Nota: Si est√°s entrenando en la misma sesi√≥n, esto pausar√° brevemente el uso de GPU.\n",
        "try:\n",
        "    model_infer, epoch_loaded = load_trained_model(device, use_latest=use_latest)\n",
        "    print(f\"‚úÖ Modelo cargado (√âpoca {epoch_loaded})\")\n",
        "\n",
        "    print(f\"\\nüß† Generando respuesta...\\n{'-'*30}\")\n",
        "    res = generate_answer(model_infer, Prompt, Max_Tokens, Temperature)\n",
        "    print(f\"üìù Pregunta: {Prompt}\")\n",
        "    print(f\"üí° Respuesta:\\n{res}\")\n",
        "    print(f\"{'-'*30}\")\n",
        "\n",
        "    # Limpieza para liberar VRAM si es necesario volver a entrenar\n",
        "    del model_infer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GijYmR1tuRVu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}